# ---------------------------------------
#
#       Master thesis, 2016/2017
#     Tereza Stanglova, A14N0115P
#
# Network architecture - size of a dataset - version 2
# ---------------------------------------

# Constants
ImageWidth          = 224
ImageHeight         = 224

NumberOfChannels        = 3
labelDim 	            = 2
mean                    = 128

# model
model = Sequential (
    ConvolutionalLayer { 96, (11:11), stride=(4:4), pad=false, init='normal', initValueScale=0.01, initBias=0.1 } : ReLU :
    MaxPoolingLayer    { (3:3), stride=(2:2) } :
    ConvolutionalLayer { 192, (5:5), pad = true, init='normal', initValueScale=0.01, initBias=0.1 } : ReLU : 
    MaxPoolingLayer    { (3:3), stride=(2:2) } :
    ConvolutionalLayer { 256, (3:3), pad = true, init='normal', initValueScale=0.01, initBias=0.1 } : ReLU :
    DenseLayer         { 4096, activation=ReLU, init='normal', initValueScale=0.005, initBias=0.1 } : Dropout :
    DenseLayer         { 4096, activation=ReLU, init='normal', initValueScale=0.005, initBias=0.1 } : Dropout :
    LinearLayer        { labelDim, init='normal', initValueScale=0.01 }
)

# inputs
features = Input { ImageWidth : ImageHeight : NumberOfChannels }
featNorm = features - Constant ( mean )
labels = Input { labelDim }

# apply model to features
z = model (featNorm)

# loss and error computation
ce       = CrossEntropyWithSoftmax  (labels, z)
errs     = ClassificationError      (labels, z)
top5Errs = ClassificationError      (labels, z, topN=5)  # only used in Eval action

# declare special nodes
featureNodes    = (features)
labelNodes      = (labels)
criterionNodes  = (ce)
evaluationNodes = (errs)
outputNodes     = (z)